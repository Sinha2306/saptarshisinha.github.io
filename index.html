<!DOCTYPE html>
<html>

  <head>
      <script async src="https://www.googletagmanager.com/gtag/js?id=UA-116924853-1"></script>
      
      <!--<script>
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());

          gtag('config', 'UA-116924853-1');
          </script>-->
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons/css/academicons.min.css">
<link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:100,200,300,400,500,600,700,800,900" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i,800,800i" rel="stylesheet">
<meta name="viewport" content="width=device-width, initial-scale=0.45, maximum-scale=1.0">
<link rel="stylesheet" type="text/css" href="style.css">
<link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
<link rel="shortcut icon" type="image/ico" href="favicon.ico" />
<link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
<link href="vendor/devicons/css/devicons.min.css" rel="stylesheet">
<link href="vendor/simple-line-icons/css/simple-line-icons.css" rel="stylesheet">


<title>Saptarshi Sinha</title>
</head>
<body align="center">
    <div id="mainBar">
        <div id="textBar">
        <header>
        <h1>
            <img src="IMG_9063.JPG" alt="Saptarshi Sinha's Photo" style="width:250px;float:right;margin-left: 5% auto;border-radius:50%" vspace="20" hspace="25">
        </h1>
        <h1>Saptarshi Sinha</h1>
        <!-- <h2 style="text-align:left">Phd Student of Computer Vision</h2> -->
        </header>
        <p class="text-justify" style="text-align:justify">I am currently a Ph.D. student working with Prof. <a href="https://dimadamen.github.io/"> Dima Damen </a> at the School of Computer Science, University of Bristol. I am part of <a href="https://uob-mavi.github.io/">MaVi</a> and
            <a href="https://vilab.blogs.bristol.ac.uk/">ViLab</a>. 
            I was previously a computer vision researcher at <a href="https://www.hitachi.com/rd/">Hitachi Research and Development</a>, Japan.
            My research interest lies in computer vision, focusing primarily on long-term video understanding. I completed my Masters from <a href="https://www.iitb.ac.in/">IIT Bombay</a> under the supervision of Prof. <a href="https://www.ee.iitb.ac.in/~sc/">Subhasis Chaudhuri</a>. </p>
        <ul class="list-inline list-social-icons mb-0">
            <!-- <li class="list-inline-item">You can find me here <span style='font-size:30px;'>&#128073;</span></li> -->
            
            <li class="list-inline-item">
                <a href="mailto:saptarshi.sinha@bristol.ac.uk">
                    <span class="fa-stack fa-lg">
                        <i class="fa fa-square fa-stack-2x"></i>
                        <i class="fa fa-envelope fa-stack-1x fa-inverse" ></i>
                    </span>
                </a>
            </li>


            <li class="list-inline-item">
                <a href="https://github.com/sinhasaptarshi">
                    <span class="fa-stack fa-lg">
                        <i class="fa fa-square fa-stack-2x"></i>
                        <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                    </span>
                </a>
            </li>

            <li class="list-inline-item">
                <a href="https://scholar.google.com/citations?user=LFXdBPoAAAAJ&hl=en">
                    <span class="fa-stack fa-lg">
                        <i class="ai ai-google-scholar-square ai-2x"></i>
                        <!-- <img src="https://scholar.google.com/favicon.ico" alt="Google Scholar Icon"> -->
                    </span>
                </a>
            </li>

            <!-- <li class="list-inline-item">
                <a href="pdfs/cv.pdf">
                    <span class="fa-stack fa-lg">
                        <i class="fa fa-square fa-stack-2x"></i>
                        <i class="fa fa-id-card fa-stack-1x fa-inverse"></i>
                    </span>
                </a>
            </li> -->

            

            <li class="list-inline-item">
                <a href="https://twitter.com/sinha_saptarshi">
                    <span class="fa-stack fa-lg">
                        <i class="fa fa-square fa-stack-2x"></i>
                        <i class="fa fa-twitter-square fa-stack-1x fa-inverse"></i>
                    </span>
                </a>
            </li>

            <li class="list-inline-item">
                <a href="https://www.linkedin.com/in/saptarshi-sinha-ab42a410b/">
                    <span class="fa-stack fa-lg">
                        <i class="fa fa-square fa-stack-2x"></i>
                        <i class="fa fa-linkedin-square fa-stack-1x fa-inverse"></i>
                    </span>
                </a>
            </li>
        </ul>
    
        <!-- <h4> Email: saptarshi (dot) sinha (at) bristol (dot) ac (dot) uk </h4> -->


    <hr>
    <h3>Activities</h3>

            <!--activities_section-->
            <ul class="text-justify" class="no-bullets" style="text-align:justify">
                <li> <span class="text-justify" class="prim-colour">August 2023</span> - <b>Paper accepted at BMVC 2023.</b> Our paper "MILA: Memory-Based Instance-Level Adaptation for Cross-Domain Object Detection" got accepted in BMVC. More information <a href="https://arxiv.org/abs/2309.01086">here</a>. 
                <li> <span class="text-justify" class="prim-colour">June 2023</span> - <b>Attended and presented at CVPR, Vancouver.</b> It was an honour presenting at CVPR and meeting new people. More information and photos <a href="https://twitter.com/dimadamen/status/1671342240604762112">here</a>.
                <li> <span class="text-justify" class="prim-colour">February 2023</span> - <b>Paper accepted at CVPR 2023.</b> Our paper "Use Your Head: Improving Long-Tail Video Recognition" got accepted in CVPR. More information <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Perrett_Use_Your_Head_Improving_Long-Tail_Video_Recognition_CVPR_2023_paper.html">here</a>. 
                <li> <span class="text-justify" class="prim-colour">September 2022</span> - <b>Started as Ph.D. at University of Bristol.</b> 
                <li> <span class="text-justify" class="prim-colour">August 2022</span> - <b>Paper accepted at WACV 2023.</b> Our paper "Difficulty-Net: Learning to Predict Difficulties for Long-Tailed Recognition" got accepted in Round-1 of WACV. More information <a href="https://arxiv.org/abs/2209.02960">here</a>. 
                <li> <span class="text-justify" class="prim-colour">August 2022</span> - <b>Published in IJCV.</b> Our paper "Class-difficulty based methods for long-tailed visual recognition" got published in IJCV. More information (<a href="https://link.springer.com/article/10.1007/s11263-022-01643-3">here</a>).
                <li> <span class="text-justify" class="prim-colour">April 2021</span> - <b>Co-organised MMAct Challenge in conjunction with ActivityNet@CVPR2021</b> - Cross-modal video action recognition/localisation challenges on the MMAct dataset. Find more information <a href="https://mmact19.github.io/challenge/">here</a>.
                <li> <span class="text-justify" class="prim-colour">November 2020</span> - <b>Presented at ACCV 2020.</b> - Nice experience presenting my first international paper.</li>
                <li> <span class="text-justify" class="prim-colour">September 2020</span> - <b>Paper Accepted at ACCV 2020. My first paper.</b> - Our paper titled "Class-wise difficulty-balanced loss for solving class-imbalance" was accepted at ACCV 2020. See more <a href="https://arxiv.org/abs/2010.01824">here</a>.</li>
                <li> <span class="text-justify" class="prim-colour">October 2019</span> - <b>Attended ICCV, Korea.</b> 
                <li> <span class="text-justify" class="prim-colour">August 2019</span> - <b>Paper published at IEICE Conferences.</b> You can find the paper <a href="https://www.ieice.org/publications/search/summary.php?id=FIT0000013722&tbl=conf_fit&lang=en">here</a>.
                <li> <span class="text-justify" class="prim-colour">September 2018</span> - <b>Started as a researcher at Hitachi R&D, Japan.</a></b> Supervised by <a href="https://scholar.google.com/citations?user=GKC6bbYAAAAJ&hl=en"> Hiroki Ohashi</a> and <a href="https://scholar.google.com/citations?user=ZIxQ5zAAAAAJ&hl=en">Katsuyuki Nakamura</a> </li>
                <!-- <li> <span class="prim-colour">August 2018</span> - <b>Graduated from IIT Mumbai</b> </li> -->
                
                </ul>

    <!-- <table>
        <tr> -->
            <!-- <td><span style='font-size:80px;'>&#9997;</span></td> -->
        <hr>
        <h3>Publications</h3>
    <!-- </tr>
    </table> -->
    <table class="researchtable" style="text-align:justify">
        <tbody>
        </tr>
        <td class=img>
            <img src="images/mila.png" style="width:300px; height:150px">
        </td>
        <td valign="top">
            <span class="text-justify" class="prim-colour">MILA: memory-based instance-level adaptation for cross-domain object detection</span>
            <br>
            Onkar Krishna, Hiroki Ohashi, <span>Saptarshi Sinha</span>
            <br>
            <em>British Machine Vision Conference (BMVC), 2023</em>
            <br>
            <strong>
                <!--<a href="https://qinghonglin.github.io/EgoVLP/">[Webpage]</a>-->
                <a href="https://arxiv.org/abs/2309.01086">[arXiv]</a>
                <a href="https://github.com/hitachi-rd-cv/MILA">[code]</a>
            </strong>
        </td>
    </tr>


            <td class=img>
                    <img src="images/long-tail-video.png" style="width:300px; height:200px">
                </td>
            <td valign="top", halign="left">
                <span class="text-justify" class="prim-colour">Use your head: Improving long-tail video recognition</span>
                <br>
                Toby Perrett,<span> Saptarshi Sinha</span>,  Tilo Burghardt, Majid Mirmehdi, Dima Damen
                <br>
                <em> IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023</em>
                <br>
                <strong>
                    <!--<a href="https://ego-exo4d-data.org/">[Webpage]</a>-->
                    <a href="https://arxiv.org/abs/2304.01143">[arXiv]</a>
                </strong>
                <strong>
                    <!--<a href="https://ego-exo4d-data.org/">[Webpage]</a>-->
                    <a href="https://github.com/tobyperrett/lmr-release">[code]</a>
                </strong>
                <strong>
                    <a href="https://github.com/tobyperrett/lmr">[page]</a>
                </strong>
            </td>

        </tr>
        <td class=img>
            <img src="images/difficulty_net.png" style="width:300px; height:200px">
        </td>
        <td valign="top">
            <span class="text-justify" class="prim-colour">Difficulty-Net: Learning to Predict Difficulty for Long-Tailed Recognition</span>
            <br>
            <span>Saptarshi Sinha</span>, Hiroki Ohashi
            <br>
            <em>IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2023</em>
            <br>
            <strong>
                <!--<a href="https://keflanagan.github.io/CliMer-TSG/">[Webpage]</a>-->
                <a href="https://arxiv.org/abs/2209.02960">[arXiv]</a>
                <a href="https://github.com/sinhasaptarshi/Difficulty_Net">[code]</a>
            </strong>
        </td>

    </tr>
    <td class=img>
        <img src="images/ijcv.png" style="width:300px; height: 200px">
    </td>
    <td valign="top">
        <span class="text-justify" class="prim-colour">Class-difficulty based methods for long-tailed visual recognition</span>
        <br>
        <span>Saptarshi Sinha</span>, Hiroki Ohashi, Katsuyuki Nakamura
        <br>
        <em>International Journal of Computer Vision (IJCV), 2022</em>
        <br>
        <strong>
            <!--<a href="https://soczech.github.io/genhowto/">[Webpage]</a>-->
            <a href="https://arxiv.org/abs/2207.14499">[arXiv]</a>
            <a href="https://github.com/sinhasaptarshi/CDB-methods">[code]</a>
        </strong>
    </td>
            
            <!--end_activities-->
    <!--research_section-->
            <tr>
                <td class=img>
                    <img src="images/accv.png" style="width:300px; height:150px">
                </td>
                <!-- <td>
                    <span style='font-size:180px;'>&#128187;</span>
                </td> -->
                <td valign="top">
                    <span class="text-justify" class="prim-colour">Class-wise difficulty-balanced loss for solving class-imbalance</span>
                    <br>
                    <span>Saptarshi Sinha</span>, Hiroki Ohashi, Katsuyuki Nakamura
                    <br>
                    <em>Asian Conference on Computer Vision (ACCV), 2020</em>
                    <br>
                    <strong>
                        <a href="https://arxiv.org/abs/2010.01824">[arXiv]</a>
                        <a href="https://github.com/sinhasaptarshi/CDB-methods">[code]</a>
                    </strong>
                </td>
            
            <!-- </tr>
                <td class=img>
                    <img src="img/ego-exo4d_intro_fig.png">
                </td>
               
             -->
            </tr>
                <td class=img>
                    <img src="images/trecvid.png" style="width:300px; height:100px">
                </td>
                <td valign="top">
                    <span class="text-justify" class="prim-colour">NII Hitachi UIT at TRECVID 2019</span>
                    <br>
                    Martin Klinkigt, Duy-Dinh Le, Atsushi Hiroike, Hung-Quoc Vo, Mohit Chabra, Vu-Minh-Hieu Dang, Quan Kong, Vinh-Tiep Nguyen, Tomokazu Murakami, Tien-Van Do 0002, Tomoaki Yoshinaga, Duy-Nhat Nguyen,<span> Sinha Saptarshi</span>, Thanh-Duc Ngo, Charles Limasanches, Tushar Agrawal, Jian Vora, Manikandan Ravikiran, Zheng Wang, Shin'ichi Satoh 
                    <br>
                    <em>TRECVID, 2019</em>
                    <br>
                    <strong>
                        <!--<a href="https://adrianofragomeni.github.io/ConTra-Context-Transformer/">[Webpage]</a>-->
                        <a href="https://openreview.net/forum?id=8TKXl2RClV">[OpenReview]</a>
                        <!--<a href="https://github.com/adrianofragomeni/ConTra">[Code]</a>-->
                    </strong>
                </td>
            
    </tbody>
    </table>
    <!--end_research-->
        </tbody>
    </table>
    </div>
    </div>
</body>